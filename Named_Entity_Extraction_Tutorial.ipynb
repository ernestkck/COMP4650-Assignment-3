{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Extraction Tutorial\n",
    "This tutorial is a slight modification of the tutorial by Sam Galen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.21.2\n",
      "Libraries succesfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "import nltk\n",
    "import scipy\n",
    "import codecs\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('sklearn version:', sklearn.__version__)\n",
    "print('Libraries succesfully loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, feature_func):\n",
    "    return [feature_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [s[-1] for s in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [s[0] for s in sent]\n",
    "\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(y_true)\n",
    "    y_pred_combined = lb.transform(y_pred)\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "            \n",
    "def word2simple_features(sent, i):\n",
    "    '''\n",
    "    This makes a simple baseline.  \n",
    "    You can add and/or remove features to get (much?) better results.\n",
    "    Experiment with it as you will need to do this for assignment.\n",
    "    '''\n",
    "    word = sent[i][0]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0, # This feature is constant for all words.\n",
    "        'word.lower()': word.lower(), # This feature is the word, ignoring case.\n",
    "        'word[-2:]': word[-2:], # This feature is the last two characters of the word (i.e. the suffix).\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'len(word)': len(word)\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            'len(word)': len(word),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            'len(word)': len(word),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "# load data and preprocess\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "    Extracting data from train file or test file. \n",
    "    path - the path of the file to extract\n",
    "    \n",
    "    return:\n",
    "        res - a list of sentences, each sentence is a\n",
    "              a list of tuples. For train file, each tuple\n",
    "              contains token and label. For test file, each\n",
    "              tuple only contains token.\n",
    "        ids - a list of ids for the corresponding token. This\n",
    "              is mainly for Kaggle submission.\n",
    "    \"\"\"\n",
    "    file = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    next(file)\n",
    "    res = []\n",
    "    ids = []\n",
    "    sent = []\n",
    "    for line in file:\n",
    "        if line != '\\n':\n",
    "            # Each line contains the position ID, the token, and (for the training set) the label.\n",
    "            parts = line.strip().split(' ')\n",
    "            sent.append(tuple(parts[1:]))\n",
    "            ids.append(parts[0])\n",
    "        else:\n",
    "            res.append(sent)\n",
    "            sent = []\n",
    "                \n",
    "    return res, ids\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a NER classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test data loaded succesfully!\n",
      "Feature Extraction done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': 'también',\n",
       "  'word[-2:]': 'én',\n",
       "  'word[-3:]': 'ién',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 7,\n",
       "  'BOS': True,\n",
       "  '+1:word.lower()': 'el',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'el',\n",
       "  'word[-2:]': 'el',\n",
       "  'word[-3:]': 'el',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'también',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'secretario',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'secretario',\n",
       "  'word[-2:]': 'io',\n",
       "  'word[-3:]': 'rio',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 10,\n",
       "  '-1:word.lower()': 'el',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'general',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'general',\n",
       "  'word[-2:]': 'al',\n",
       "  'word[-3:]': 'ral',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 7,\n",
       "  '-1:word.lower()': 'secretario',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'general',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'la',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'la',\n",
       "  'word[-2:]': 'la',\n",
       "  'word[-3:]': 'la',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'asociación',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'asociación',\n",
       "  'word[-2:]': 'ón',\n",
       "  'word[-3:]': 'ión',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 10,\n",
       "  '-1:word.lower()': 'la',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'española',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'española',\n",
       "  'word[-2:]': 'la',\n",
       "  'word[-3:]': 'ola',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 8,\n",
       "  '-1:word.lower()': 'asociación',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'española',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'operadores',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'operadores',\n",
       "  'word[-2:]': 'es',\n",
       "  'word[-3:]': 'res',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 10,\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'operadores',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'productos',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'productos',\n",
       "  'word[-2:]': 'os',\n",
       "  'word[-3:]': 'tos',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 9,\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'petrolíferos',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'petrolíferos',\n",
       "  'word[-2:]': 'os',\n",
       "  'word[-3:]': 'ros',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 12,\n",
       "  '-1:word.lower()': 'productos',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 1,\n",
       "  '-1:word.lower()': 'petrolíferos',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'aurelio',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'aurelio',\n",
       "  'word[-2:]': 'io',\n",
       "  'word[-3:]': 'lio',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 7,\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'ayala',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ayala',\n",
       "  'word[-2:]': 'la',\n",
       "  'word[-3:]': 'ala',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 5,\n",
       "  '-1:word.lower()': 'aurelio',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 1,\n",
       "  '-1:word.lower()': 'ayala',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'ha',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ha',\n",
       "  'word[-2:]': 'ha',\n",
       "  'word[-3:]': 'ha',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'negado',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'negado',\n",
       "  'word[-2:]': 'do',\n",
       "  'word[-3:]': 'ado',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 6,\n",
       "  '-1:word.lower()': 'ha',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'la',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'la',\n",
       "  'word[-2:]': 'la',\n",
       "  'word[-3:]': 'la',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'negado',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'existencia',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'existencia',\n",
       "  'word[-2:]': 'ia',\n",
       "  'word[-3:]': 'cia',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 10,\n",
       "  '-1:word.lower()': 'la',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'existencia',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'cualquier',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'cualquier',\n",
       "  'word[-2:]': 'er',\n",
       "  'word[-3:]': 'ier',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 9,\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'tipo',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'tipo',\n",
       "  'word[-2:]': 'po',\n",
       "  'word[-3:]': 'ipo',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 4,\n",
       "  '-1:word.lower()': 'cualquier',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'tipo',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'acuerdos',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'acuerdos',\n",
       "  'word[-2:]': 'os',\n",
       "  'word[-3:]': 'dos',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 8,\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'sobre',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'sobre',\n",
       "  'word[-2:]': 're',\n",
       "  'word[-3:]': 'bre',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 5,\n",
       "  '-1:word.lower()': 'acuerdos',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'los',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'los',\n",
       "  'word[-2:]': 'os',\n",
       "  'word[-3:]': 'los',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 3,\n",
       "  '-1:word.lower()': 'sobre',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'precios',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'precios',\n",
       "  'word[-2:]': 'os',\n",
       "  'word[-3:]': 'ios',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 7,\n",
       "  '-1:word.lower()': 'los',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 1,\n",
       "  '-1:word.lower()': 'precios',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'afirmando',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'afirmando',\n",
       "  'word[-2:]': 'do',\n",
       "  'word[-3:]': 'ndo',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 9,\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'que',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'que',\n",
       "  'word[-2:]': 'ue',\n",
       "  'word[-3:]': 'que',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 3,\n",
       "  '-1:word.lower()': 'afirmando',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'únicamente',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'únicamente',\n",
       "  'word[-2:]': 'te',\n",
       "  'word[-3:]': 'nte',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 10,\n",
       "  '-1:word.lower()': 'que',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'es',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'es',\n",
       "  'word[-2:]': 'es',\n",
       "  'word[-3:]': 'es',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'únicamente',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'la',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'la',\n",
       "  'word[-2:]': 'la',\n",
       "  'word[-3:]': 'la',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'es',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'cotización',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'cotización',\n",
       "  'word[-2:]': 'ón',\n",
       "  'word[-3:]': 'ión',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 10,\n",
       "  '-1:word.lower()': 'la',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'internacional',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'internacional',\n",
       "  'word[-2:]': 'al',\n",
       "  'word[-3:]': 'nal',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 13,\n",
       "  '-1:word.lower()': 'cotización',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'la',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'la',\n",
       "  'word[-2:]': 'la',\n",
       "  'word[-3:]': 'la',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'internacional',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'que',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'que',\n",
       "  'word[-2:]': 'ue',\n",
       "  'word[-3:]': 'que',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 3,\n",
       "  '-1:word.lower()': 'la',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'pone',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'pone',\n",
       "  'word[-2:]': 'ne',\n",
       "  'word[-3:]': 'one',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 4,\n",
       "  '-1:word.lower()': 'que',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 2,\n",
       "  '-1:word.lower()': 'pone',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'acuerdo',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'acuerdo',\n",
       "  'word[-2:]': 'do',\n",
       "  'word[-3:]': 'rdo',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 7,\n",
       "  '-1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'a',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'a',\n",
       "  'word[-2:]': 'a',\n",
       "  'word[-3:]': 'a',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 1,\n",
       "  '-1:word.lower()': 'acuerdo',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'todos',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'todos',\n",
       "  'word[-2:]': 'os',\n",
       "  'word[-3:]': 'dos',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 5,\n",
       "  '-1:word.lower()': 'a',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'los',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'los',\n",
       "  'word[-2:]': 'os',\n",
       "  'word[-3:]': 'los',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 3,\n",
       "  '-1:word.lower()': 'todos',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'países',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'países',\n",
       "  'word[-2:]': 'es',\n",
       "  'word[-3:]': 'ses',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 6,\n",
       "  '-1:word.lower()': 'los',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'len(word)': 1,\n",
       "  '-1:word.lower()': 'países',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  'EOS': True}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train and test data\n",
    "train_data, train_ids = extract_data('train')\n",
    "test_data, test_ids = extract_data('test')\n",
    "\n",
    "# Load true labels for test data\n",
    "test_labels = list(pd.read_csv('test_ground_truth').loc[:, 'label'])\n",
    "\n",
    "print('Train and Test data loaded succesfully!')\n",
    "\n",
    "# Feature extraction using the word2simple_features function\n",
    "train_features = [sent2features(s, feature_func=word2simple_features) for s in train_data]\n",
    "train_labels = [sent2labels(s) for s in train_data]\n",
    "test_features = [sent2features(s, feature_func=word2simple_features) for s in test_data]\n",
    "\n",
    "trainer = pycrfsuite.Trainer(algorithm='lbfgs', verbose=False)\n",
    "for xseq, yseq in zip(train_features, train_labels):\n",
    "    trainer.append(xseq, yseq)\n",
    "print('Feature Extraction done!')    \n",
    "\n",
    "# Explore the extracted features    \n",
    "sent2features(train_data[0], word2simple_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 0.1,   # coefficient for L1 penalty\n",
    "    'c2': 0.1,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done :)\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('ner-esp.model')\n",
    "\n",
    "print('Training done :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with your NER model\n",
    "Make predictions and evaluate your model on the test set.\n",
    "To use your NER model, create pycrfsuite.Tagger, open the model, and use the \"tag\" method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.82      0.82      0.82      2036\n",
      "       I-LOC       0.73      0.77      0.75       725\n",
      "      B-MISC       0.61      0.76      0.68       706\n",
      "      I-MISC       0.62      0.65      0.63      1205\n",
      "       B-ORG       0.85      0.87      0.86      3136\n",
      "       I-ORG       0.84      0.81      0.82      2291\n",
      "       B-PER       0.90      0.91      0.90      1865\n",
      "       I-PER       0.94      0.94      0.94      1632\n",
      "\n",
      "   micro avg       0.82      0.84      0.83     13596\n",
      "   macro avg       0.79      0.82      0.80     13596\n",
      "weighted avg       0.82      0.84      0.83     13596\n",
      " samples avg       0.10      0.10      0.10     13596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('ner-esp.model')\n",
    "test_pred = [tagger.tag(xseq) for xseq in test_features]\n",
    "test_pred = [s for w in test_pred for s in w]\n",
    "\n",
    "## Print evaluation\n",
    "print(bio_classification_report(test_pred, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 {'num': 100, 'scores': {}, 'loss': 8014.592385, 'feature_norm': 143.515279, 'error_norm': 576.824024, 'active_features': 28145, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.178}\n"
     ]
    }
   ],
   "source": [
    "print (len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what the classifier has learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "I-MISC -> I-MISC  4.700132\n",
      "I-ORG  -> I-ORG   4.368079\n",
      "B-ORG  -> I-ORG   4.291800\n",
      "I-LOC  -> I-LOC   3.938575\n",
      "B-LOC  -> I-LOC   3.811628\n",
      "B-PER  -> I-PER   3.713824\n",
      "B-MISC -> I-MISC  3.676691\n",
      "O      -> O       3.540892\n",
      "I-PER  -> I-PER   3.222277\n",
      "O      -> B-ORG   1.580806\n",
      "O      -> B-MISC  1.063171\n",
      "O      -> B-PER   0.655464\n",
      "O      -> B-LOC   0.532967\n",
      "B-ORG  -> O       -0.122448\n",
      "I-LOC  -> O       -0.133611\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-ORG  -> I-LOC   -3.541001\n",
      "B-MISC -> I-ORG   -3.640333\n",
      "B-MISC -> B-MISC  -3.669086\n",
      "B-PER  -> B-ORG   -3.685735\n",
      "I-ORG  -> I-MISC  -3.696378\n",
      "I-MISC -> B-ORG   -3.729409\n",
      "B-ORG  -> B-ORG   -3.739666\n",
      "I-ORG  -> I-PER   -3.892700\n",
      "I-ORG  -> I-LOC   -4.104742\n",
      "I-ORG  -> B-LOC   -4.748850\n",
      "B-PER  -> B-PER   -5.236093\n",
      "O      -> I-ORG   -5.487109\n",
      "O      -> I-MISC  -5.678040\n",
      "O      -> I-PER   -5.773493\n",
      "O      -> I-LOC   -6.374917\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of a person name (B-PER) will be followed by a token inside person name (I-PER). Also note O -> B-LOC are penalized.\n",
    "\n",
    "## Check the state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "8.814194 B-ORG  word.lower():efe-cantabria\n",
      "7.599136 O      word.lower():y\n",
      "7.438450 B-ORG  word.lower():psoe-progresistas\n",
      "7.059294 O      word.lower():a\n",
      "5.752124 O      BOS\n",
      "5.322476 I-PER  -1:word.lower():antoñete\n",
      "5.189838 B-ORG  word.lower():bnp-paribas\n",
      "5.040786 B-LOC  word.lower():líbano\n",
      "4.941653 B-ORG  word.lower():petrobras\n",
      "4.840271 B-PER  word.lower():franca\n",
      "4.797150 B-MISC word.lower():firagran\n",
      "4.731994 B-MISC word.lower():justicia\n",
      "4.731486 B-ORG  word[-2:]:-e\n",
      "4.680997 B-ORG  word.lower():gales\n",
      "4.637589 B-ORG  +1:word.lower():deutsche\n",
      "4.503533 I-ORG  -1:word.lower():l\n",
      "4.279195 B-LOC  -1:word.lower():cantabria\n",
      "4.275674 B-ORG  word.lower():telefónica\n",
      "4.265343 O      word[-2:]:63\n",
      "4.254343 B-ORG  word.lower():eu-ecologista\n",
      "\n",
      "Top negative:\n",
      "-2.393790 O      word.lower():bosque\n",
      "-2.403469 B-PER  -1:word.lower():las\n",
      "-2.408018 O      word.lower():2000\n",
      "-2.452281 O      word[-3:]:and\n",
      "-2.461585 O      word.lower():061\n",
      "-2.519016 O      -1:word.lower():coi\n",
      "-2.670017 I-MISC BOS\n",
      "-2.672534 B-MISC word[-2:]:de\n",
      "-2.734665 O      -1:word.lower():españolas\n",
      "-2.767981 O      -1:word.lower():;\n",
      "-2.823857 O      word[-2:]:nd\n",
      "-2.839162 B-LOC  word[-3:]:la\n",
      "-2.931273 B-ORG  word[-2:]:de\n",
      "-3.033122 B-PER  word.lower():la\n",
      "-3.168678 B-PER  -1:word.lower():del\n",
      "-3.271318 I-PER  -1:word.lower():san\n",
      "-3.905062 O      -1:word.lower():celebrarán\n",
      "-4.208683 O      word[-2:]:om\n",
      "-5.656423 O      word.isupper()\n",
      "-8.348629 O      word.istitle()\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
