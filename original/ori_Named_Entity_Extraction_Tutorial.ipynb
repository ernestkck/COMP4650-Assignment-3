{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Extraction Tutorial\n",
    "This tutorial is a slight modification of the tutorial by Sam Galen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.21.2\n",
      "Libraries succesfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "import nltk\n",
    "import scipy\n",
    "import codecs\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('sklearn version:', sklearn.__version__)\n",
    "print('Libraries succesfully loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, feature_func):\n",
    "    return [feature_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [s[-1] for s in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [s[0] for s in sent]\n",
    "\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(y_true)\n",
    "    y_pred_combined = lb.transform(y_pred)\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "            \n",
    "def word2simple_features(sent, i):\n",
    "    '''\n",
    "    This makes a simple baseline.  \n",
    "    You can add and/or remove features to get (much?) better results.\n",
    "    Experiment with it as you will need to do this for assignment.\n",
    "    '''\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, # This feature is constant for all words.\n",
    "        'word.lower()': word.lower(), # This feature is the word, ignoring case.\n",
    "        'word[-2:]': word[-2:], # This feature is the last two characters of the word (i.e. the suffix).\n",
    "    }\n",
    "    if i == 0:\n",
    "        features['BOS'] = True # Mark the beginning of sentence.\n",
    "        \n",
    "    if i == len(sent)-1:\n",
    "        features['EOS'] = True # Mark the end of sentence.\n",
    "\n",
    "    return features\n",
    "\n",
    "# load data and preprocess\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "    Extracting data from train file or test file. \n",
    "    path - the path of the file to extract\n",
    "    \n",
    "    return:\n",
    "        res - a list of sentences, each sentence is a\n",
    "              a list of tuples. For train file, each tuple\n",
    "              contains token and label. For test file, each\n",
    "              tuple only contains token.\n",
    "        ids - a list of ids for the corresponding token. This\n",
    "              is mainly for Kaggle submission.\n",
    "    \"\"\"\n",
    "    file = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    next(file)\n",
    "    res = []\n",
    "    ids = []\n",
    "    sent = []\n",
    "    for line in file:\n",
    "        if line != '\\n':\n",
    "            # Each line contains the position ID, the token, and (for the training set) the label.\n",
    "            parts = line.strip().split(' ')\n",
    "            sent.append(tuple(parts[1:]))\n",
    "            ids.append(parts[0])\n",
    "        else:\n",
    "            res.append(sent)\n",
    "            sent = []\n",
    "                \n",
    "    return res, ids\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a NER classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test data loaded succesfully!\n",
      "Feature Extraction done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0, 'word.lower()': 'también', 'word[-2:]': 'én', 'BOS': True},\n",
       " {'bias': 1.0, 'word.lower()': 'el', 'word[-2:]': 'el'},\n",
       " {'bias': 1.0, 'word.lower()': 'secretario', 'word[-2:]': 'io'},\n",
       " {'bias': 1.0, 'word.lower()': 'general', 'word[-2:]': 'al'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'la', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'asociación', 'word[-2:]': 'ón'},\n",
       " {'bias': 1.0, 'word.lower()': 'española', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'operadores', 'word[-2:]': 'es'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'productos', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'petrolíferos', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': ',', 'word[-2:]': ','},\n",
       " {'bias': 1.0, 'word.lower()': 'aurelio', 'word[-2:]': 'io'},\n",
       " {'bias': 1.0, 'word.lower()': 'ayala', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': ',', 'word[-2:]': ','},\n",
       " {'bias': 1.0, 'word.lower()': 'ha', 'word[-2:]': 'ha'},\n",
       " {'bias': 1.0, 'word.lower()': 'negado', 'word[-2:]': 'do'},\n",
       " {'bias': 1.0, 'word.lower()': 'la', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'existencia', 'word[-2:]': 'ia'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'cualquier', 'word[-2:]': 'er'},\n",
       " {'bias': 1.0, 'word.lower()': 'tipo', 'word[-2:]': 'po'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'acuerdos', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'sobre', 'word[-2:]': 're'},\n",
       " {'bias': 1.0, 'word.lower()': 'los', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'precios', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': ',', 'word[-2:]': ','},\n",
       " {'bias': 1.0, 'word.lower()': 'afirmando', 'word[-2:]': 'do'},\n",
       " {'bias': 1.0, 'word.lower()': 'que', 'word[-2:]': 'ue'},\n",
       " {'bias': 1.0, 'word.lower()': 'únicamente', 'word[-2:]': 'te'},\n",
       " {'bias': 1.0, 'word.lower()': 'es', 'word[-2:]': 'es'},\n",
       " {'bias': 1.0, 'word.lower()': 'la', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'cotización', 'word[-2:]': 'ón'},\n",
       " {'bias': 1.0, 'word.lower()': 'internacional', 'word[-2:]': 'al'},\n",
       " {'bias': 1.0, 'word.lower()': 'la', 'word[-2:]': 'la'},\n",
       " {'bias': 1.0, 'word.lower()': 'que', 'word[-2:]': 'ue'},\n",
       " {'bias': 1.0, 'word.lower()': 'pone', 'word[-2:]': 'ne'},\n",
       " {'bias': 1.0, 'word.lower()': 'de', 'word[-2:]': 'de'},\n",
       " {'bias': 1.0, 'word.lower()': 'acuerdo', 'word[-2:]': 'do'},\n",
       " {'bias': 1.0, 'word.lower()': 'a', 'word[-2:]': 'a'},\n",
       " {'bias': 1.0, 'word.lower()': 'todos', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'los', 'word[-2:]': 'os'},\n",
       " {'bias': 1.0, 'word.lower()': 'países', 'word[-2:]': 'es'},\n",
       " {'bias': 1.0, 'word.lower()': '.', 'word[-2:]': '.', 'EOS': True}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train and test data\n",
    "train_data, train_ids = extract_data('train')\n",
    "test_data, test_ids = extract_data('test')\n",
    "\n",
    "# Load true labels for test data\n",
    "test_labels = list(pd.read_csv('test_ground_truth').loc[:, 'label'])\n",
    "\n",
    "print('Train and Test data loaded succesfully!')\n",
    "\n",
    "# Feature extraction using the word2simple_features function\n",
    "train_features = [sent2features(s, feature_func=word2simple_features) for s in train_data]\n",
    "train_labels = [sent2labels(s) for s in train_data]\n",
    "test_features = [sent2features(s, feature_func=word2simple_features) for s in test_data]\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(train_features, train_labels):\n",
    "    trainer.append(xseq, yseq)\n",
    "print('Feature Extraction done!')    \n",
    "\n",
    "# Explore the extracted features    \n",
    "sent2features(train_data[0], word2simple_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the classifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 100.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done :)\n",
      "Wall time: 6.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('ner-esp.model')\n",
    "\n",
    "print('Training done :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with your NER model\n",
    "Make predictions and evaluate your model on the test set.\n",
    "To use your NER model, create pycrfsuite.Tagger, open the model, and use the \"tag\" method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.02      0.97      0.04        38\n",
      "      B-MISC       0.00      0.17      0.00         6\n",
      "      I-MISC       0.00      0.00      0.00      1872\n",
      "       B-ORG       0.13      0.96      0.23       428\n",
      "       I-ORG       0.00      0.13      0.00        39\n",
      "       B-PER       0.02      0.78      0.03        40\n",
      "       I-PER       0.03      0.88      0.06        58\n",
      "\n",
      "   micro avg       0.04      0.22      0.07      2481\n",
      "   macro avg       0.03      0.56      0.05      2481\n",
      "weighted avg       0.03      0.22      0.04      2481\n",
      " samples avg       0.00      0.00      0.00      2481\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program_Files\\Anaconda3\\envs\\comp4650_2019\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Program_Files\\Anaconda3\\envs\\comp4650_2019\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('ner-esp.model')\n",
    "test_pred = [tagger.tag(xseq) for xseq in test_features]\n",
    "test_pred = [s for w in test_pred for s in w]\n",
    "\n",
    "## Print evaluation\n",
    "print(bio_classification_report(test_pred, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 {'num': 50, 'scores': {}, 'loss': 100700.567394, 'feature_norm': 17.166387, 'error_norm': 1023.086536, 'active_features': 158, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.113}\n"
     ]
    }
   ],
   "source": [
    "print (len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what the classifier has learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-PER  -> I-PER   4.910164\n",
      "B-MISC -> I-MISC  4.361037\n",
      "I-ORG  -> I-ORG   4.288836\n",
      "I-MISC -> I-MISC  4.272204\n",
      "B-ORG  -> I-ORG   3.881272\n",
      "I-LOC  -> I-LOC   3.695069\n",
      "I-PER  -> I-PER   3.513057\n",
      "B-LOC  -> I-LOC   3.467629\n",
      "O      -> B-ORG   1.969413\n",
      "O      -> O       1.610842\n",
      "O      -> B-LOC   1.551324\n",
      "O      -> B-PER   1.270429\n",
      "O      -> B-MISC  1.074687\n",
      "B-LOC  -> O       0.494818\n",
      "B-ORG  -> O       0.405177\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-LOC  -> I-LOC   3.467629\n",
      "O      -> B-ORG   1.969413\n",
      "O      -> O       1.610842\n",
      "O      -> B-LOC   1.551324\n",
      "O      -> B-PER   1.270429\n",
      "O      -> B-MISC  1.074687\n",
      "B-LOC  -> O       0.494818\n",
      "B-ORG  -> O       0.405177\n",
      "I-PER  -> O       0.140915\n",
      "I-ORG  -> O       -0.000078\n",
      "I-MISC -> O       -0.167976\n",
      "O      -> I-LOC   -1.803157\n",
      "O      -> I-PER   -1.829670\n",
      "O      -> I-MISC  -1.966156\n",
      "O      -> I-ORG   -2.171883\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of a person name (B-PER) will be followed by a token inside person name (I-PER). Also note O -> B-LOC are penalized.\n",
    "\n",
    "## Check the state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "2.897303 O      EOS\n",
      "2.750286 O      word.lower():el\n",
      "2.681898 B-ORG  word.lower():efe\n",
      "2.645789 B-ORG  word.lower():gobierno\n",
      "2.338800 O      bias\n",
      "2.304716 O      word.lower():una\n",
      "2.083749 O      word.lower():en\n",
      "2.068063 O      word.lower():la\n",
      "2.064000 O      word.lower():,\n",
      "2.064000 O      word[-2:]:,\n",
      "1.898316 O      word.lower():que\n",
      "1.896922 I-PER  word[-2:]:ez\n",
      "1.710038 O      word.lower():al\n",
      "1.636413 O      word.lower():y\n",
      "1.550425 B-LOC  BOS\n",
      "1.520263 B-ORG  word[-2:]:FE\n",
      "1.496896 B-LOC  word[-2:]:id\n",
      "1.443095 O      word.lower():para\n",
      "1.437732 O      word.lower():por\n",
      "1.428778 O      word.lower():a\n",
      "\n",
      "Top negative:\n",
      "-0.039968 O      word[-2:]:io\n",
      "-0.040217 O      word[-2:]:co\n",
      "-0.085603 O      word[-2:]:ca\n",
      "-0.122929 B-PER  word.lower():la\n",
      "-0.180838 I-ORG  bias\n",
      "-0.207791 B-MISC bias\n",
      "-0.217876 B-PER  word[-2:]:de\n",
      "-0.222371 B-PER  word[-2:]:ón\n",
      "-0.248464 O      word[-2:]:ga\n",
      "-0.292959 O      word.lower():gobierno\n",
      "-0.315779 B-ORG  word[-2:]:es\n",
      "-0.367054 B-LOC  word[-2:]:de\n",
      "-0.412633 O      word[-2:]:na\n",
      "-0.518082 O      word[-2:]:FE\n",
      "-0.583376 O      word[-2:]:pa\n",
      "-0.746322 O      word[-2:]:ia\n",
      "-0.770622 B-ORG  word[-2:]:de\n",
      "-0.882811 O      word.lower():efe\n",
      "-0.892935 O      word[-2:]:ña\n",
      "-0.934733 O      word[-2:]:ez\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
