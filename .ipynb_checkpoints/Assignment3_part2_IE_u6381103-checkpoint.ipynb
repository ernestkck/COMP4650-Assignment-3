{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Part 2: IE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this assignment, the task is to code a Named Entity Recognizer (NER) application in Python using the CRFsuite library.\n",
    "\n",
    "It is recommended you complete the Named_Entity_Extraction_Tutorial.ipynb tutorial before attemping this.\n",
    "\n",
    "Your tasks for this assignment are to:\n",
    "1. Build a NER classifier following the tutorial.\n",
    "2. Improve the performance of your NER classifier.\n",
    "3. Answer three written assignments.\n",
    "\n",
    "* Write answers in this notebook file, and upload the file to Wattle submission site. **Please rename and submit jupyter notebook file (Assignment5.ipynb) to your_uid.ipynb (e.g. u6000001.ipynb) with your written answers therein**. Do not upload any other files to Wattle except this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Question 1 (2 points) Build a NER model <a id='Task1'></a> </span>\n",
    "### Part A (1.5 marks)\n",
    "\n",
    "* Build a NER model using the train and test data files.\n",
    "* You can use the code provided in [tutorial sheet](Named_Entity_Extraction_Tutorial.ipynb) \n",
    "* Try changing the feature extraction, model hyper parameters, or other settings in order to improve your model performance.\n",
    "* Marks will be awarded based on how well your model performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "import nltk\n",
    "import scipy\n",
    "import codecs\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, feature_func):\n",
    "    return [feature_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [s[-1] for s in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [s[0] for s in sent]\n",
    "\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(y_true)\n",
    "    y_pred_combined = lb.transform(y_pred)\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "            \n",
    "def word2simple_features(sent, i):\n",
    "    '''\n",
    "    This makes a simple baseline.  \n",
    "    You can add and/or remove features to get (much?) better results.\n",
    "    Experiment with it as you will need to do this for assignment.\n",
    "    '''\n",
    "    word = sent[i][0]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0, # This feature is constant for all words.\n",
    "        'word.lower()': word.lower(), # This feature is the word, ignoring case.\n",
    "        'word[-2:]': word[-2:], # This feature is the last two characters of the word (i.e. the suffix).\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'contains_digit': any(c.isdigit() for c in word),\n",
    "        'len(word)': len(word)\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:word[-2:]': word1[-2:],\n",
    "            '-1:word[-3:]': word1[-3:],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:word[-2:]': word1[-2:],\n",
    "            '+1:word[-3:]': word1[-3:],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "# load data and preprocess\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "    Extracting data from train file or test file. \n",
    "    path - the path of the file to extract\n",
    "    \n",
    "    return:\n",
    "        res - a list of sentences, each sentence is a\n",
    "              a list of tuples. For train file, each tuple\n",
    "              contains token and label. For test file, each\n",
    "              tuple only contains token.\n",
    "        ids - a list of ids for the corresponding token. This\n",
    "              is mainly for Kaggle submission.\n",
    "    \"\"\"\n",
    "    file = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    next(file)\n",
    "    res = []\n",
    "    ids = []\n",
    "    sent = []\n",
    "    for line in file:\n",
    "        if line != '\\n':\n",
    "            # Each line contains the position ID, the token, and (for the training set) the label.\n",
    "            parts = line.strip().split(' ')\n",
    "            sent.append(tuple(parts[1:]))\n",
    "            ids.append(parts[0])\n",
    "        else:\n",
    "            res.append(sent)\n",
    "            sent = []\n",
    "                \n",
    "    return res, ids            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test data loaded succesfully!\n",
      "Feature Extraction done!\n"
     ]
    }
   ],
   "source": [
    "# Load train and test data\n",
    "train_data, train_ids = extract_data('train')\n",
    "test_data, test_ids = extract_data('test')\n",
    "\n",
    "# Load true labels for test data\n",
    "test_labels = list(pd.read_csv('test_ground_truth').loc[:, 'label'])\n",
    "\n",
    "print('Train and Test data loaded succesfully!')\n",
    "\n",
    "# Feature extraction using the word2simple_features function\n",
    "train_features = [sent2features(s, feature_func=word2simple_features) for s in train_data]\n",
    "train_labels = [sent2labels(s) for s in train_data]\n",
    "test_features = [sent2features(s, feature_func=word2simple_features) for s in test_data]\n",
    "\n",
    "trainer = pycrfsuite.Trainer(algorithm='lbfgs', verbose=False)\n",
    "for xseq, yseq in zip(train_features, train_labels):\n",
    "    trainer.append(xseq, yseq)\n",
    "print('Feature Extraction done!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 0.1,   # coefficient for L1 penalty\n",
    "    'c2': 0.1,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done :)\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('ner-esp.model')\n",
    "print('Training done :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.82      0.82      0.82      2052\n",
      "       I-LOC       0.73      0.77      0.75       722\n",
      "      B-MISC       0.64      0.74      0.68       755\n",
      "      I-MISC       0.63      0.64      0.63      1219\n",
      "       B-ORG       0.84      0.87      0.86      3115\n",
      "       I-ORG       0.83      0.81      0.82      2272\n",
      "       B-PER       0.89      0.92      0.90      1844\n",
      "       I-PER       0.94      0.94      0.94      1626\n",
      "\n",
      "   micro avg       0.82      0.84      0.83     13605\n",
      "   macro avg       0.79      0.81      0.80     13605\n",
      "weighted avg       0.82      0.84      0.83     13605\n",
      " samples avg       0.10      0.10      0.10     13605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('ner-esp.model')\n",
    "test_pred = [tagger.tag(xseq) for xseq in test_features]\n",
    "test_pred = [s for w in test_pred for s in w]\n",
    "\n",
    "## Print evaluation\n",
    "print(bio_classification_report(test_pred, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B (0.5 marks)\n",
    "\n",
    "Briefly explain what changes to your model you tried and how these changes affected the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added the following in feature extraction:\n",
    "- Added another suffix (last 3 letters)\n",
    "- Check if it is all upper case\n",
    "- Check if the first letter is upper case\n",
    "- Check if it contains a digit \n",
    "- Length of the word\n",
    "- Also look at the word around it (the previous word and the following word)\n",
    "\n",
    "Changed the follwing hyper parameters:\n",
    "- Changed the coefficients for L1 and L2 penalty to 0.1\n",
    "- Increased the max iterations to 100\n",
    "\n",
    "These changes have significantly improved the model's performance in all the evaluation scores. I tried adding more features and using other training algorithms (such as SGD with L2), but there was no visible improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Written Part (3 points) </span>\n",
    "\n",
    "Answer briefly and concisely the following questions.\n",
    "Check [this](https://sourceforge.net/p/jupiter/wiki/markdown_syntax/#md_ex_lists) if you are not familiar with markdown syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (0.5 point)\n",
    "Think of three relevant baselines for the Named Entity Classification task.\n",
    "Provide answers using bullet list with 3 items. Give a short description of each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random assignment\n",
    "\n",
    "The simple baseline is to randomly assign the label to every word, or assign the same label to every word. If your performance is worse than that, then there is not much purpose for your model.\n",
    "\n",
    "2. A simple feature set\n",
    "\n",
    "A simple version of the NER model that you are currently using that only uses a simple feature set. For example the original feature extraction that is given to us in our tutorial. It basically only uses the last 2 letters of the word as a feature. We could add a few more basic features such as isupper() and istitle() as the baseline.\n",
    "\n",
    "3. An established CRF based NER classifier\n",
    "\n",
    "A NER model that uses conditional random field and is used and approved by the community, such as the Stanford CRF NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1.5 point)\n",
    "How does Maximal Marginal Relevance (MMR) address redundancy issues? (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MMR is the weighted combination of (a) $\\lambda$ multipled by the similarity score between the query and the item and (b) $1-\\lambda$ multipled by the similarity score between two selected items. \n",
    "\n",
    "    If $\\lambda=1$, then it will only use the similarity score between the query and the item like a standard relevance-ranked list. If $\\lambda$ is a small value close to 0, it will give results that are different from each other.\n",
    "\n",
    "    As such, the parameter $\\lambda$ can be set to a value such as 0.3 to ensure that the returned items are diverse.\n",
    "High MMR means an item is relevant to the query and contains minimal similarity to previous selected items.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you tell MMR that \"Sydney\" and \"Melbourne\" are cities? (0.5 points)\n",
    "\n",
    "- Use a word embedding (e.g. GloVe) and a similarity measure (e.g. cosine similarity) such that these are close to \"city\" but not very close to each other. That way the query \"city\" with $\\lambda=0.5$ may return both \"Sydney\" and \"Melbourne\" in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you tell MMR that \"solar panels\" and \"photovoltaic cells\" have similar meaning? (0.5 points)\n",
    "\n",
    "- Use a word embedding (e.g. GloVe) and a similarity measure (e.g. cosine similarity) such that these they have high similarity scores. That way with a $\\lambda=0.2$ may see the two terms as redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1 point)\n",
    "\n",
    "Imagine you are developing an extractive text summarization tool using HMM.\n",
    "\n",
    "What are the hidden states and the observations of the HMM model? (0.5 point)\n",
    "\n",
    "Which algorithm is used to compute the probability of a particular observation sequence? (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The hidden states represent whether a sentence is in the summary state or non-summary state. The observations are the sentences that are in the summary.\n",
    "- Forward algorithm is used to compute the probability of a particular observation sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
